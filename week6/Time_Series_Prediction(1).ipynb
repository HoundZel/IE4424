{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE4424 Lab Time_Series_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "This lab experiment is inspired by Usman Malik's Stack Abuse article.\n",
    "\n",
    "You can check out the original post at https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Training a time series predictor\n",
    "\n",
    "A time series data is a sequential data indexed in time order.\n",
    "Often we would like to predict the future behaviour of a time series.\n",
    "For example, we may be interested in predicting the weather, share prices, sales, etc. \n",
    "Long Short-Term Memory (LSTM) model can be applied to perform time series prediction. \n",
    "\n",
    "In this exercise, we will use the ``flights`` dataset from the seaborn library.\n",
    "The dataset contains 3 columns: ``year``, ``month``, and ``passengers``.\n",
    "Where the ``year`` and ``month`` columns refer to the particular year and month of the monthly flight record, respectively.\n",
    "The ``passengers`` column records the total number of passengers that took the flights in that month.\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1) Load and normalize the training and testing datasets using ``StandardScalar``\n",
    "   \n",
    "2) Define an LSTM model\n",
    "\n",
    "3) Define a loss function\n",
    "\n",
    "4) Train the network on the training data\n",
    "\n",
    "5) Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading the flights dataset\n",
    "\n",
    "\n",
    "Loading the ``flights`` data from ``seaborn`` is strightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "flights = sns.load_dataset('flights')\n",
    "print(type(flights)) # the dataset is loaded as pandas dataframe\n",
    "flights.describe(include = 'all')\n",
    "\n",
    "# save the data as comma-separated values (csv) to facilitate viewing of the raw data\n",
    "flights.to_csv('flights_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Visualizing the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(flights['passengers'], 'k')\n",
    "plt.title('flight records')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Normalizing the data and converting to tensor\n",
    "\n",
    "In a regression problem such as this, it is beneficial to normalize the data. It allows the model to converge faster and avoid having large losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "passengers = flights['passengers'].values.astype(float) # convert the passengers column to float \n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(passengers.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Generating the sequences\n",
    "Since we have 144 months (i.e., 12 years) of flight data, we use 132 months (i.e., 11 years) for training and the remaining 12 months (i.e., 1 year) for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create windowed sequence tuples of (x,y), with moving window of step size = 1\n",
    "def windowed_sequences(data, window_size=12, test_size=12):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-window_size-test_size+1):\n",
    "        j = i + window_size\n",
    "        xi = data[i:j]\n",
    "        yi = data[j]\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    return x, y\n",
    "        \n",
    "test_size = 12\n",
    "seqeunces, targets = windowed_sequences(normalized_data, 12, test_size)\n",
    "\n",
    " # Convert the sequences to float type tensors\n",
    "train_x = torch.Tensor(seqeunces[:-1]).type(torch.float)\n",
    "train_y = torch.Tensor(targets[:-1]).type(torch.float) \n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Constructing the iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pytorch data loading utility to construct iterable over the dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item],  self.y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "train_set = CustomDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_set, batch_size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Defining the LSTM model\n",
    "\n",
    "Since we only have 1 input and 1 output value which is the number of passengers, we set the input dimension (input_dim) and output dimension (output_dim) as 1.\n",
    "hidden_dim defines the number of features in the hidden state of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeSeriesPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=200, output_dim=1):\n",
    "        super(TimeSeriesPredictor,self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, self.hidden_dim,\n",
    "                            batch_first=True)\n",
    "                \n",
    "        self.fc1 = nn.Linear(self.hidden_dim, 100)\n",
    "        self.fc2 = nn.Linear(100, output_dim)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "\n",
    "        lstm_out, (hn, cn) = self.lstm(input_seq)\n",
    "        predictions = F.relu(self.fc1(hn.view(-1, self.hidden_dim)))\n",
    "        predictions = self.fc2(predictions)\n",
    "        return predictions\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  #2024 modification CPU > GPU\n",
    "model = TimeSeriesPredictor()\n",
    "model.to(device)#2024 modification CPU > GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Printing the network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Using Torchinfo to view a summary of the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo #2024 modification CPU > GPU\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(24,12,1)) # (batch_size, sequence_length, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.9 Defining loss function and optimizer\n",
    "\n",
    "The loss function is defined as Mean Squared Error loss and optimizer is Adam optimizer. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.10 Training the network\n",
    "\n",
    "We loop over our data iterator, and feed the inputs to the\n",
    "network and optimize the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "model.to('cpu')\n",
    "t1 = time.time()\n",
    "loss_list = []\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        seq, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    loss_list.append(running_loss/len(train_loader))\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'epoch: {epoch+1:3} loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "t2 = time.time()\n",
    "print('Finished Training')\n",
    "print('Training time:'+str(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1 Plotting of loss function\n",
    "\n",
    "In the previous cell, we have saved the loss function at different iterations in the loss_list.\n",
    "\n",
    "Now, plot the loss function versus the iteration number using matplotlib.\n",
    "\n",
    "You can find the tutorial of plotting figures using matplotlib here:\n",
    "https://matplotlib.org/3.3.3/gallery/lines_bars_and_markers/simple_plot.html#sphx-glr-gallery-lines-bars-and-markers-simple-plot-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.1.1 Example of using matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of a simple plot\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for plotting\n",
    "t = np.arange(0.0, 2.0, 0.01)\n",
    "s = 1 + np.sin(2 * np.pi * t)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, s)\n",
    "\n",
    "ax.set(xlabel='time (s)', ylabel='voltage (mV)',\n",
    "       title='About as simple as it gets, folks')\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.1.2 Plotting the loss function vs interation number (To do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training loss curve\n",
    "# To do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11 Evaluating network performance on the test data\n",
    "\n",
    "We have trained the network for several passes over the training dataset.\n",
    "But we need to check if the network has learned anything at all.\n",
    "\n",
    "We will check this by predicting the next time-step value that the neural network\n",
    "outputs, and checking it against the ground truth.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(132, 144, 1)\n",
    "plt.title('Month vs Passenger')\n",
    "plt.ylabel('Passengers')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(x_axis, passengers[-test_size:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Perform predictions using the trained model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_sl12 = []\n",
    "with torch.no_grad():\n",
    "    for i in range(test_size):\n",
    "        outputs = model(test_seq.unsqueeze(dim=0)) # unsqueeze to add a dimension to accomodate the batch processing\n",
    "        test_seq = torch.cat((test_seq,outputs),dim=0)[-12:]\n",
    "        preds_sl12.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2 Evaluating network performance \n",
    "\n",
    "In this section, we are going to evaluate the accuracy of the model in predicting future values. Since we are predicting more than 1 value, we will use the Mean Absolute Percentage Error (MAPE) to measure the difference between the predicted value and the ground truth value. \n",
    "\n",
    "### E1.2.1 Computing MAPE (To do)\n",
    "\n",
    "Finish the code and calculate the MAPE for 12 future predictions. They can be computed using API from scikit-learn:\n",
    "\n",
    "MAPE: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_percentage_error.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "denormalized_preds_sl12 = scaler.inverse_transform(np.array(preds_sl12).reshape(-1, 1))\n",
    "\n",
    "# To do\n",
    "mape = \n",
    "\n",
    "print('MAPE: %.2f%%' % (mape*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.2.2 Plotting the predictions against the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(132, 144, 1)\n",
    "plt.title('Month vs Passenger')\n",
    "plt.ylabel('Passengers')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(passengers, 'k')\n",
    "plt.plot(x_axis,denormalized_preds_sl12, 'r^--', label='seq_len_12')\n",
    "plt.axvline(x=132, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.2.3 Zoom in to the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(132, 144, 1)\n",
    "plt.title('Month vs Passenger')\n",
    "plt.ylabel('Passengers')\n",
    "plt.grid(True)\n",
    "plt.autoscale(axis='x', tight=True)\n",
    "plt.plot(x_axis,passengers[-test_size:], 'ko-', label='ground_truth')\n",
    "plt.plot(x_axis,denormalized_preds_sl12, 'r^--', label='seq_len_12')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E1.3 Study of different window/sequence lengths\n",
    "\n",
    "In this exercise, you will learn to train the network with the input of a different window/sequence length.\n",
    "\n",
    "In the previous section, we used a sequence length of 12 for the time series input. You will use a sequence length of 24 in this exercise and compare the evaluation results for these two window/sequence lengths.\n",
    "\n",
    "### E1.3.1 Prepare the new training iterables (To do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n",
    "seqeunces, targets =\n",
    "train_x = torch.Tensor(seqeunces[:-1]).type(torch.float)\n",
    "train_y = torch.Tensor(targets[:-1]).type(torch.float)\n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)\n",
    "\n",
    "train_set = CustomDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_set, batch_size=24)\n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.3.2 Re-initializing the network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesPredictor()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.3.3 Training the network with the new input sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#2024 modification CPU > GPU\n",
    "model.to(device)#2024 modification CPU > GPU\n",
    "t1 = time.time()\n",
    "loss_list = []\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        seq, labels = data\n",
    "        seq, labels = seq.to(device), labels.to(device)#2024 modification CPU > GPU\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    loss_list.append(running_loss/len(train_loader))\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'epoch: {epoch+1:3} loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "t2 = time.time()\n",
    "print('Finished Training')\n",
    "print('Training time:'+str(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.3.4 Perform predictions\n",
    "\n",
    "Perform predictions using the model trained on inputs sequence length = 24."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#2024 modification CPU > GPU\n",
    "model.to(device)#2024 modification CPU > GPU\n",
    "preds_sl24 = []\n",
    "with torch.no_grad():\n",
    "    for i in range(test_size):\n",
    "        outputs = model(test_seq.unsqueeze(dim=0)) # unsqueeze to add a dimension to accomodate the batch processing\n",
    "        test_seq = torch.cat((test_seq,outputs),dim=0)[-24:]\n",
    "        preds_sl24.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.3.5 Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Check if preds_sl24 is a list, and convert to a NumPy array\n",
    "if isinstance(preds_sl24, list):\n",
    "    # If it's a list of tensors, we need to convert each tensor to NumPy\n",
    "    preds_sl24 = np.concatenate([x.cpu().detach().numpy().flatten() if isinstance(x, torch.Tensor) else np.array(x).flatten() for x in preds_sl24])#2024 modification CPU > GPU\n",
    "\n",
    "# Now preds_sl24 is a 1D NumPy array, reshape it to (n_samples, 1) for the scaler\n",
    "denormalized_preds_sl24 = scaler.inverse_transform(preds_sl24.reshape(-1, 1))#2024 modification CPU > GPU\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(passengers[-test_size:], denormalized_preds_sl24)\n",
    "\n",
    "# Print the MAPE as percentage\n",
    "print(f'MAPE: {mape * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.3.6 Plotting the results for both sequence lengths (To do)\n",
    "\n",
    "\n",
    "Re-plot the plot in E1.2.3 by adding the input-sequence-length-24 prediction. \n",
    "\n",
    "Specifically, plot the ground truth (in black and label it as 'ground truth'), the sequence-length-12 prediction (in red dotted line with ^ marker and label it as 'seq_len_12'), and the sequence-length-24 prediction (in green dotted line with x marker and label it as 'seq_len_24')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4 Study of different hidden dimensions\n",
    "\n",
    "In this exercise, you will learn to train the network using a different hidden layer dimension.\n",
    "\n",
    "In the previous section, we used a hidden dimension of 200. You will use a hidden dimension of 400 in this exercise and compare the results for these 2 hidden dimensions. We will keep the window/sequence length of 12.\n",
    "\n",
    "### E1.4.1 Re-initializing the network with different hidden dimensions (To do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n",
    "model = \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # lower learning may be better for more complex model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.4.2 View a summary of the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(24,12,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.4.3 Reinitialize the training iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "test_size = 12\n",
    "seqeunces, targets = windowed_sequences(normalized_data, window_size, test_size)\n",
    "train_x = torch.Tensor(seqeunces[:-1]).type(torch.float)\n",
    "train_y = torch.Tensor(targets[:-1]).type(torch.float)\n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)\n",
    "\n",
    "train_set = CustomDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_set, batch_size=24)\n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.4.4 Training the network with new hidden dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)#2024 modification CPU > GPU\n",
    "t1 = time.time()\n",
    "loss_list = []\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        seq, labels = data\n",
    "        seq, labels = seq.to(device), labels.to(device)#2024 modification CPU > GPU\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    loss_list.append(running_loss/len(train_loader))\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'epoch: {epoch+1:3} loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "t2 = time.time()\n",
    "print('Finished Training')\n",
    "print('Training time:'+str(t2-t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.4.5 Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#2024 modification CPU > GPU\n",
    "model.to(device)#2024 modification CPU > GPU\n",
    "preds_hd400 = []\n",
    "with torch.no_grad():\n",
    "    for i in range(test_size):\n",
    "        outputs = model(test_seq.unsqueeze(dim=0)) # unsqueeze to add a dimension to accomodate the batch processing\n",
    "        test_seq = torch.cat((test_seq,outputs),dim=0)[-12:]\n",
    "        preds_hd400.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.4.6 Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# If preds_hd400 is a list, convert it to a NumPy array\n",
    "if isinstance(preds_hd400, list):\n",
    "    # Convert the list into a 1D NumPy array\n",
    "    preds_hd400 = np.concatenate([x.cpu().detach().numpy().flatten() if isinstance(x, torch.Tensor) else np.array(x).flatten() for x in preds_hd400])#2024 modification CPU > GPU\n",
    "\n",
    "# If preds_hd400 is a tensor (after conversion from the list), move it to CPU\n",
    "elif isinstance(preds_hd400, torch.Tensor):\n",
    "    preds_hd400 = preds_hd400.cpu().detach().numpy() #2024 modification CPU > GPU\n",
    "\n",
    "# Reshape the array to a column vector (if needed) before inverse transform\n",
    "denormalized_preds_hd400 = scaler.inverse_transform(preds_hd400.reshape(-1, 1))#2024 modification CPU > GPU\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(passengers[-test_size:], denormalized_preds_hd400)\n",
    "\n",
    "# Print the MAPE as percentage\n",
    "print(f'MAPE: {mape * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1.4.7 Plotting the results in the same plot (To do)\n",
    "\n",
    "Re-plot the plot in E1.2.3 by adding the hidden-dimension-400 prediction. \n",
    "\n",
    "Specifically, plot the the ground truth (in black and label it as 'ground truth'), the hidden-dimension-200 prediction (in red dotted line with ^ marker and label it as 'hid_dim_200'), and the hidden-dimension-400 prediction (in green dotted line with x marker and label it as 'hid_dim_400')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ie4414)",
   "language": "python",
   "name": "ie4414"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
