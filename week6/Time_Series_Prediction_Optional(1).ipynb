{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IE4424 Lab Time_Series_Prediction_Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgment\n",
    "\n",
    "This lab experiment is inspired by Usman Malik's Stack Abuse article.\n",
    "\n",
    "You can check out the original post at https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Training a time series predictor\n",
    "\n",
    "A time series data is a sequential data indexed in time order.\n",
    "Often we would like to predict the future behaviour of a time series.\n",
    "For example, we may be interested in predicting the weather, share prices, sales, etc. \n",
    "Long Short-Term Memory (LSTM) model can be applied to perform time series prediction. \n",
    "\n",
    "In this exercise, we will use the ``flights`` dataset from the seaborn library.\n",
    "The dataset contains 3 columns: ``year``, ``month``, and ``passengers``.\n",
    "Where the ``year`` and ``month`` columns refer to the particular year and month of the monthly flight record, respectively.\n",
    "The ``passengers`` column records the total number of passengers that took the flights in that month.\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "1) Load and normalize the training and testing datasets using ``StandardScalar``\n",
    "   \n",
    "2) Define an LSTM model\n",
    "\n",
    "3) Define a loss function\n",
    "\n",
    "4) Train the network on the training data\n",
    "\n",
    "5) Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(0)\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading the flights dataset\n",
    "\n",
    "\n",
    "Loading the ``flights`` data from ``seaborn`` is strightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "flights = sns.load_dataset('flights')\n",
    "print(type(flights)) # the dataset is loaded as pandas dataframe\n",
    "flights.describe(include = 'all')\n",
    "\n",
    "# save the data as comma-separated values (csv) to facilitate viewing of the raw data\n",
    "flights.to_csv('flights_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Visualizing the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(flights['passengers'])\n",
    "plt.title('flight records')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Normalizing the data and converting to tensor\n",
    "\n",
    "In a regression problem such as this, it is beneficial to normalize the data. It allows the model to converge faster and avoid having large losses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "passengers = flights['passengers'].values.astype(float) # convert the passengers column to float \n",
    "scaler = StandardScaler()\n",
    "normalized_data = scaler.fit_transform(passengers.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Generating the sequences\n",
    "Since we have 144 months (i.e., 12 years) of flight data, we use 132 months (i.e., 11 years) for training and the remaining 12 months (i.e., 1 year) for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create windowed sequence tuples of (x,y), with moving window of step size = 1\n",
    "def windowed_sequences(data, window_size=12, test_size=12):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(data)-window_size-test_size+1):\n",
    "        j = i + window_size\n",
    "        xi = data[i:j]\n",
    "        yi = data[j]\n",
    "        x.append(xi)\n",
    "        y.append(yi)\n",
    "    return x, y\n",
    "        \n",
    "test_size = 12\n",
    "seqeunces, targets = windowed_sequences(normalized_data, 12, test_size)\n",
    "\n",
    " # Convert the sequences to float type tensors\n",
    "train_x = torch.Tensor(seqeunces[:-1]).type(torch.float)\n",
    "train_y = torch.Tensor(targets[:-1]).type(torch.float) \n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Constructing the iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pytorch data loading utility to construct iterable over the dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item],  self.y[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "train_set = CustomDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_set, batch_size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Exercise\n",
    "\n",
    "So far we have considered different configurations for a single LSTM layer. \n",
    "\n",
    "Similar to the CNN, additional layers can be added to the LSTM to make the model deeper and adding levels of abstraction. This is achieved by stacking multilayers of LSTM on top of each other. \n",
    "\n",
    "In addition, on some sequential data problems, having the LSTM model learn the input sequences from both forward and backward directions can be beneficial. The backward direction LSTM processes the input in reverse chronological order. To consider both forward and backward information the hidden states from both directions are concatenated.\n",
    "\n",
    "In this part, you will modify the network into a bidirectional multilayer LSTM.\n",
    "Pytorch LSTM module has provided a convenient wrapper to facilitate the construction of bidirectional and multilayer LSTM: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "Note that the hidden states of the LSTM module may change according to the configurations (i.e., bidirectional would double the hidden state size at each layer). Tune the model to find the most suitable hyperparameters for your bidirectional multilayer LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Defining the bidirectional multilayer LSTM model (To do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=200, output_dim=1, num_layers=1):\n",
    "        super(TimeSeriesPredictor,self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # To do\n",
    "        self.lstm = nn.LSTM(input_dim, self.hidden_dim, num_layers=num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(self.hidden_dim*2, 100)\n",
    "        self.fc2 = nn.Linear(100, output_dim)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "\n",
    "        lstm_out, (hn, cn) = self.lstm(input_seq)\n",
    "        # To do\n",
    "        hn = \n",
    "        predictions = F.relu(self.fc1(hn))\n",
    "        predictions = self.fc2(predictions)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Initializing the network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TimeSeriesPredictor(num_layers=2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001) # adapt to increased model complexity by lowering lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Print the network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Initialize the training iterables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 12\n",
    "test_size = 12\n",
    "seqeunces, targets = windowed_sequences(normalized_data, window_size, test_size)\n",
    "train_x = torch.Tensor(seqeunces[:-1]).type(torch.float)\n",
    "train_y = torch.Tensor(targets[:-1]).type(torch.float)\n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)\n",
    "\n",
    "train_set = CustomDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_set, batch_size=12) # Reduce batch_size to prevent out of memory\n",
    "test_seq = torch.Tensor(seqeunces[-1]).type(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")#2024 modification CPU > GPU\n",
    "model.to(device)#2024 modification CPU > GPU\n",
    "t1 = time.time()\n",
    "loss_list = []\n",
    "for epoch in range(1000):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        seq, labels = data\n",
    "        seq, labels = seq.to(device), labels.to(device)#2024 modification CPU > GPU\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    loss_list.append(running_loss/len(train_loader))\n",
    "    if epoch % 100 == 99:\n",
    "        print(f'epoch: {epoch+1:3} loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "t2 = time.time()\n",
    "print('Finished Training')\n",
    "print('Training time:'+str(t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = np.arange(len(loss_list))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(iterations, loss_list)\n",
    "ax.set(xlabel='Iterations', ylabel='Loss value',\n",
    "       title='loss curve versus the iteration')\n",
    "ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Perform predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_bdml = []\n",
    "with torch.no_grad():\n",
    "    for i in range(test_size):\n",
    "        outputs = model(test_seq.unsqueeze(dim=0)) # unsqueeze to add a dimension to accomodate the batch processing\n",
    "        test_seq = torch.cat((test_seq,outputs),dim=0)[-12:]\n",
    "        preds_bdml.append(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# Move each tensor in preds_bdml to CPU and convert to numpy, then reshape\n",
    "denormalized_preds_bdml = np.array([output.cpu().numpy() for output in preds_bdml]).reshape(-1, 1)#2024 modification CPU > GPU\n",
    "\n",
    "# Apply inverse transformation using the scaler\n",
    "denormalized_preds_bdml = scaler.inverse_transform(denormalized_preds_bdml)#2024 modification CPU > GPU\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = mean_absolute_percentage_error(passengers[-test_size:], denormalized_preds_bdml)\n",
    "\n",
    "# Print the MAPE as percentage\n",
    "print('MAPE: %.2f%%' % (mape * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Plotting the results in the same plot (To do)\n",
    "\n",
    "Plot the the ground truth (in black and label it as 'ground truth') and the bidirectional multilayer LSTM prediction (in green dotted line with x marker and label it as 'bdml')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
